{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.10 |Anaconda, Inc.| (default, May  7 2020, 19:46:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas: 1.0.3\n",
      "Numpy: 1.18.1\n",
      "Sklearn: 0.22.1\n",
      "XGBoost: 1.1.1\n",
      "Keras: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a random seed for reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "import sklearn\n",
    "import xgboost\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('Pandas: {}'.format(pd.__version__))\n",
    "print('Numpy: {}'.format(np.__version__))\n",
    "print('Sklearn: {}'.format(sklearn.__version__))\n",
    "print('XGBoost: {}'.format(xgboost.__version__))\n",
    "print('Keras: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_and_test(path):\n",
    "\n",
    "    \"\"\"Loads training features, training labels, testing features, and testing features\n",
    "    Parameters:\n",
    "        path (str) -- a single directory path containing all four datasets\n",
    "    \"\"\"\n",
    "\n",
    "    train_features = pd.read_csv(path + 'train_features.csv', index_col=0)\n",
    "    train_labels = pd.read_csv(path + 'train_labels.csv', index_col=0)\n",
    "    test_features = pd.read_csv(path + 'test_features.csv', index_col=0)\n",
    "    test_labels = pd.read_csv(path + 'test_labels.csv', index_col=0)\n",
    "\n",
    "    return train_features, train_labels.values.ravel(), test_features, test_labels.values.ravel()\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_train_and_test('../data/split/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all optimized models from the models folder\n",
    "\n",
    "dirName = '../models/'\n",
    "\n",
    "fileList = list()\n",
    "dirList = list()\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os. walk(dirName):\n",
    "    for file in filenames:\n",
    "        if '.sav' in file:\n",
    "            fileList.append(os.path. join(dirpath, file))\n",
    "\n",
    "modelList = list()\n",
    "\n",
    "for file in fileList:\n",
    "    model = pickle.load(open(file, 'rb'))\n",
    "    modelList.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_roc_curves(model, X_test, Y_test):\n",
    "    # predict probabilities\n",
    "    lr_probs = model.predict_proba(X_test)\n",
    "\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = [round(x,2) for x in lr_probs[:, 1]]\n",
    "    \n",
    "    n_bootstraps = 1000\n",
    "    rng_seed = 42# control reproducibility\n",
    "    bootstrapped_scores = []\n",
    "    fprList = []\n",
    "    tprList = []\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(lr_probs), len(lr_probs))\n",
    "        if len(np.unique(np.array(Y_test)[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            # to be defined: reject the sample\n",
    "            continue\n",
    "\n",
    "        labels = np.array(Y_test)[indices]\n",
    "        predictions = np.array(lr_probs)[indices]\n",
    "\n",
    "        # calculate scores and curve\n",
    "        bootstrapped_scores.append(sklearn.metrics.roc_auc_score(labels, predictions))\n",
    "        lr_fpr, lr_tpr, _ = sklearn.metrics.roc_curve(labels, predictions, drop_intermediate=True)\n",
    "        \n",
    "        fprList.append(lr_fpr)\n",
    "        tprList.append(lr_tpr)\n",
    "    \n",
    "    return fprList, tprList, bootstrapped_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_roc_curves(models, names, X_test, Y_test, bootstrap=True):\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    \n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(Y_test))]\n",
    "    \n",
    "    for i,model in enumerate(models):\n",
    "        \n",
    "        # predict probabilities\n",
    "        lr_probs = model.predict_proba(X_test)\n",
    "        \n",
    "        # keep probabilities for the positive outcome only\n",
    "        lr_probs = lr_probs[:, 1]\n",
    "        \n",
    "        # calculate scores and curve\n",
    "        lr_auc = sklearn.metrics.roc_auc_score(Y_test, lr_probs)\n",
    "        lr_fpr, lr_tpr, _ = sklearn.metrics.roc_curve(Y_test, lr_probs)\n",
    "        \n",
    "        if bootstrap:\n",
    "            fpr, tpr, scores = bootstrap_roc_curves(model, X_test, Y_test)\n",
    "            \n",
    "            scores.sort()\n",
    "            sorted = np.array(scores)\n",
    "        \n",
    "        # plot the roc curve for the model\n",
    "        plt.plot(lr_fpr, lr_tpr, color=cmap(0.1*i), marker='', label='{} (AUC = {} (95% CI {} to {}))'.format(names[i], round(lr_auc,3), round(sorted[25], 3), round(sorted[975],3)))\n",
    "    \n",
    "    # plot the roc curve for the no skill model\n",
    "    ns_fpr, ns_tpr, _ = sklearn.metrics.roc_curve(Y_test, ns_probs)\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', color='black', label='No Skill (AUC = 0.500)')\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "    # title\n",
    "    plt.title('ROC AUC Scores by Algorithm Type')\n",
    "    # show the legend\n",
    "    plt.legend(loc = 'lower right')\n",
    "    \n",
    "    #save the figure\n",
    "    plt.savefig('../reports/figures/ROC_AUC_comparison.png')\n",
    "    \n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [file.split('/')[-1].rstrip('.sav') for file in fileList]\n",
    "generate_roc_curves(modelList, index, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "                   learning_rate=1.9553972700565248, n_estimators=16,\n",
      "                   random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93       270\n",
      "           1       0.13      0.07      0.09        30\n",
      "\n",
      "    accuracy                           0.86       300\n",
      "   macro avg       0.52      0.51      0.51       300\n",
      "weighted avg       0.82      0.86      0.84       300\n",
      "\n",
      "257, 13, 28, 2\n",
      "\n",
      "Sensitivity: 0.06666666666666667\n",
      "Specificity: 0.9518518518518518\n",
      "PPV: 0.13333333333333333\n",
      "NPV: 0.9017543859649123\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced', criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.00017372854537759253,\n",
      "                       min_impurity_split=None, min_samples_leaf=9,\n",
      "                       min_samples_split=4,\n",
      "                       min_weight_fraction_leaf=0.09200176096726041,\n",
      "                       presort='deprecated', random_state=0, splitter='best')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85       270\n",
      "           1       0.16      0.37      0.22        30\n",
      "\n",
      "    accuracy                           0.75       300\n",
      "   macro avg       0.54      0.58      0.54       300\n",
      "weighted avg       0.84      0.75      0.79       300\n",
      "\n",
      "213, 57, 19, 11\n",
      "\n",
      "Sensitivity: 0.36666666666666664\n",
      "Specificity: 0.7888888888888889\n",
      "PPV: 0.16176470588235295\n",
      "NPV: 0.9181034482758621\n",
      "\n",
      "\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=6.439275088909716, loss='deviance',\n",
      "                           max_depth=27, max_features='auto',\n",
      "                           max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.09364736540475367,\n",
      "                           min_impurity_split=None, min_samples_leaf=2,\n",
      "                           min_samples_split=5,\n",
      "                           min_weight_fraction_leaf=0.29602978323891826,\n",
      "                           n_estimators=178, n_iter_no_change=None,\n",
      "                           presort='deprecated', random_state=0,\n",
      "                           subsample=0.7416582575852527, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.60      0.73       270\n",
      "           1       0.16      0.67      0.25        30\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.55      0.63      0.49       300\n",
      "weighted avg       0.86      0.60      0.68       300\n",
      "\n",
      "161, 109, 10, 20\n",
      "\n",
      "Sensitivity: 0.6666666666666666\n",
      "Specificity: 0.5962962962962963\n",
      "PPV: 0.15503875968992248\n",
      "NPV: 0.9415204678362573\n",
      "\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0007626595718254758, batch_size=3,\n",
      "              beta_1=0.7075634415715792, beta_2=0.9555562770335876,\n",
      "              early_stopping=True, epsilon=1e-08, hidden_layer_sizes=(128, 64),\n",
      "              learning_rate='constant', learning_rate_init=0.08117955536691433,\n",
      "              max_fun=15000, max_iter=228, momentum=0.7879443185393082,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.2699852101699892, random_state=None, shuffle=True,\n",
      "              solver='lbfgs', tol=0.0015738447293094154,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92       270\n",
      "           1       0.19      0.17      0.18        30\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.55      0.54      0.55       300\n",
      "weighted avg       0.84      0.85      0.84       300\n",
      "\n",
      "249, 21, 25, 5\n",
      "\n",
      "Sensitivity: 0.16666666666666666\n",
      "Specificity: 0.9222222222222223\n",
      "PPV: 0.19230769230769232\n",
      "NPV: 0.9087591240875912\n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
      "                       class_weight='balanced_subsample', criterion='gini',\n",
      "                       max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "                       max_samples=None,\n",
      "                       min_impurity_decrease=0.008135280338330211,\n",
      "                       min_impurity_split=None, min_samples_leaf=6,\n",
      "                       min_samples_split=6,\n",
      "                       min_weight_fraction_leaf=0.04357678838454903,\n",
      "                       n_estimators=253, n_jobs=8, oob_score=False,\n",
      "                       random_state=0, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90       270\n",
      "           1       0.25      0.40      0.31        30\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.59      0.63      0.60       300\n",
      "weighted avg       0.86      0.82      0.84       300\n",
      "\n",
      "234, 36, 18, 12\n",
      "\n",
      "Sensitivity: 0.4\n",
      "Specificity: 0.8666666666666667\n",
      "PPV: 0.25\n",
      "NPV: 0.9285714285714286\n",
      "\n",
      "\n",
      "SVC(C=0.13869718045614998, break_ties=False, cache_size=200,\n",
      "    class_weight='balanced', coef0=0.0, decision_function_shape='ovr', degree=3,\n",
      "    gamma='scale', kernel='linear', max_iter=-1, probability=True,\n",
      "    random_state=None, shrinking=True, tol=0.003993171050079279, verbose=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.80      0.87       270\n",
      "           1       0.26      0.63      0.37        30\n",
      "\n",
      "    accuracy                           0.78       300\n",
      "   macro avg       0.61      0.72      0.62       300\n",
      "weighted avg       0.88      0.78      0.82       300\n",
      "\n",
      "216, 54, 11, 19\n",
      "\n",
      "Sensitivity: 0.6333333333333333\n",
      "Specificity: 0.8\n",
      "PPV: 0.2602739726027397\n",
      "NPV: 0.9515418502202643\n",
      "\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5094033724526472,\n",
      "              gamma=1.164079119558058, gpu_id=-1, importance_type='gain',\n",
      "              interaction_constraints='', learning_rate=0.08308916654519226,\n",
      "              max_delta_step=0, max_depth=17, min_child_weight=9.2655229178349,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=19, n_jobs=0,\n",
      "              num_parallel_tree=1, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=40.72214163571579,\n",
      "              subsample=0.6724271235300521, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.85       270\n",
      "           1       0.18      0.40      0.25        30\n",
      "\n",
      "    accuracy                           0.76       300\n",
      "   macro avg       0.55      0.60      0.55       300\n",
      "weighted avg       0.85      0.76      0.79       300\n",
      "\n",
      "215, 55, 18, 12\n",
      "\n",
      "Sensitivity: 0.4\n",
      "Specificity: 0.7962962962962963\n",
      "PPV: 0.1791044776119403\n",
      "NPV: 0.9227467811158798\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def scores(Y_test, y_pred):\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(Y_test, y_pred).ravel()\n",
    "    print('{}, {}, {}, {}\\n'.format(tn, fp, fn, tp))\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    PPV = tp / (tp + fp)\n",
    "    NPV = tn / (tn + fn)\n",
    "\n",
    "    print('Sensitivity: {}'.format(sensitivity))\n",
    "    print('Specificity: {}'.format(specificity))\n",
    "    print('PPV: {}'.format(PPV))\n",
    "    print('NPV: {}'.format(NPV))\n",
    "\n",
    "    return None\n",
    "\n",
    "curves = []\n",
    "\n",
    "for model in modelList:\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(model)\n",
    "    print(classification_report(Y_test, y_pred))\n",
    "    scores(Y_test, y_pred)\n",
    "    curves.append(sklearn.metrics.roc_curve(Y_test, y_pred))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NSQIP]",
   "language": "python",
   "name": "conda-env-NSQIP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
