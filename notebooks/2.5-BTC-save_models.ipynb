{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.10 |Anaconda, Inc.| (default, May  7 2020, 19:46:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas: 1.0.3\n",
      "Numpy: 1.18.1\n",
      "Sklearn: 0.22.1\n",
      "XGBoost: 1.1.1\n",
      "Keras: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a random seed for reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "import sklearn\n",
    "import xgboost\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import os\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('Pandas: {}'.format(pd.__version__))\n",
    "print('Numpy: {}'.format(np.__version__))\n",
    "print('Sklearn: {}'.format(sklearn.__version__))\n",
    "print('XGBoost: {}'.format(xgboost.__version__))\n",
    "print('Keras: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the processed dataset\n",
    "df = pd.read_csv('../data/processed/NSQIP_Clean2.csv')\n",
    "data = df\n",
    "data = data.drop(columns=['index.1', 'index', 'Unnamed: 0']).copy()\n",
    "# Replace missing values with median values\n",
    "data['AGE'].replace(np.NaN, data['AGE'].median(), inplace=True)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data.drop(columns=['READMISSION1'])\n",
    "labels = data.READMISSION1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size = 300, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of all optimization results files\n",
    "dirName = '../reports/optimization/'\n",
    "\n",
    "fileList = list()\n",
    "dirList = list()\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os. walk(dirName):\n",
    "    for file in filenames:\n",
    "        if '.csv' in file:\n",
    "            fileList.append(os.path. join(dirpath, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name, hyperparameters):\n",
    "    \n",
    "    \"\"\" Loads the appropriate sklearn model from a model name and hyperparameters \"\"\"\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    if name == 'AdaBoost':\n",
    "        model = AdaBoostClassifier(**hyperparameters)\n",
    "    elif name == 'DecisionTree':\n",
    "        model = DecisionTreeClassifier(**hyperparameters)\n",
    "    elif name == 'KMeans':\n",
    "        model = KNeighborsClassifier(**hyperparameters)\n",
    "    elif name == 'MLP':\n",
    "        model = MLPClassifier(**hyperparameters)\n",
    "    elif name == 'RandomForest':\n",
    "        model = RandomForestClassifier(**hyperparameters)\n",
    "    elif name == 'SVC':\n",
    "        model = SVC(**hyperparameters, probability=True)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBClassifier(**hyperparameters)\n",
    "    else:\n",
    "        print('Unkown model name')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for file in fileList:\n",
    "    #load hyperparameter optimization files\n",
    "    results = pd.read_csv(file)\n",
    "    \n",
    "    # find the model_name from the filename\n",
    "    file_split = file.split('/')[3]\n",
    "    model_name = file_split.split('\\\\')[0]\n",
    "\n",
    "    new_results = results.copy()\n",
    "\n",
    "    # String to dictionary\n",
    "    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n",
    "\n",
    "    # Sort with best values on top\n",
    "    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "\n",
    "    # Use best hyperparameters to create a model\n",
    "    hyperparameters = new_results.loc[0, 'hyperparameters']\n",
    "    \n",
    "    # load the appropriate model and fit on training data\n",
    "    model = load_model(model_name, hyperparameters)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # create output filename\n",
    "    new_dir = '../models/'\n",
    "    filename = new_dir + model_name + '.sav'\n",
    "    \n",
    "    # save the trained model\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NSQIP]",
   "language": "python",
   "name": "conda-env-NSQIP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
