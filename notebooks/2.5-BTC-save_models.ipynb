{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-aafd1ccaac92>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-aafd1ccaac92>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    from ../src.util import util\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a random seed for reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "import sklearn\n",
    "import xgboost\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import os\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('Pandas: {}'.format(pd.__version__))\n",
    "print('Numpy: {}'.format(np.__version__))\n",
    "print('Sklearn: {}'.format(sklearn.__version__))\n",
    "print('XGBoost: {}'.format(xgboost.__version__))\n",
    "print('Keras: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the processed dataset\n",
    "df = pd.read_csv('../data/processed/NSQIP_Clean2.csv')\n",
    "data = df\n",
    "data = data.drop(columns=['index.1', 'index', 'Unnamed: 0']).copy()\n",
    "# Replace missing values with median values\n",
    "data['AGE'].replace(np.NaN, data['AGE'].median(), inplace=True)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data.drop(columns=['READMISSION1'])\n",
    "labels = data.READMISSION1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size = 300, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_and_test(path):\n",
    "\n",
    "    \"\"\"Loads training features, training labels, testing features, and testing features\n",
    "    Parameters:\n",
    "        path (str) -- a single directory path containing all four datasets\n",
    "    \"\"\"\n",
    "\n",
    "    train_features = pd.read_csv(path + 'train_features.csv', index_col=0)\n",
    "    train_labels = pd.read_csv(path + 'train_labels.csv', index_col=0)\n",
    "    test_features = pd.read_csv(path + 'test_features.csv', index_col=0)\n",
    "    test_labels = pd.read_csv(path + 'test_labels.csv', index_col=0)\n",
    "\n",
    "    return train_features, train_labels.values.ravel(), test_features, test_labels.values.ravel()\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_train_and_test('../data/split/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of all optimization results files\n",
    "dirName = '../reports/optimization/'\n",
    "\n",
    "fileList = list()\n",
    "dirList = list()\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os. walk(dirName):\n",
    "    for file in filenames:\n",
    "        if '.csv' in file:\n",
    "            fileList.append(os.path. join(dirpath, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name, hyperparameters):\n",
    "    \n",
    "    \"\"\" Loads the appropriate sklearn model from a model name and hyperparameters \"\"\"\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    \n",
    "    if name == 'AdaBoost':\n",
    "        model = AdaBoostClassifier(**hyperparameters)\n",
    "    elif name == 'DecisionTree':\n",
    "        model = DecisionTreeClassifier(**hyperparameters)\n",
    "    elif name == 'KMeans':\n",
    "        model = KNeighborsClassifier(**hyperparameters)\n",
    "    elif name == 'MLP':\n",
    "        model = MLPClassifier(**hyperparameters)\n",
    "    elif name == 'RandomForest':\n",
    "        model = RandomForestClassifier(**hyperparameters)\n",
    "    elif name == 'SVC':\n",
    "        model = SVC(**hyperparameters, probability=True)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBClassifier(**hyperparameters)\n",
    "    elif name == 'GradientBoosting':\n",
    "        model = GradientBoostingClassifier(**hyperparameters)\n",
    "    else:\n",
    "        print('Unkown model name')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../reports/optimization/AdaBoost\\2020-07-17_bayes_test.csv\n",
      "../reports/optimization/DecisionTree\\2020-07-17_bayes_test.csv\n",
      "../reports/optimization/GradientBoosting\\2020-07-18_bayes_test.csv\n",
      "../reports/optimization/MLP\\2020-07-16_bayes_test.csv\n",
      "../reports/optimization/RandomForest\\2020-07-17_bayes_test.csv\n",
      "../reports/optimization/SVC\\2020-07-16_bayes_test.csv\n",
      "../reports/optimization/XGBoost\\2020-07-17_bayes_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for file in fileList:\n",
    "    #load hyperparameter optimization files\n",
    "    results = pd.read_csv(file)\n",
    "    \n",
    "    # find the model_name from the filename\n",
    "    file_split = file.split('/')[3]\n",
    "    model_name = file_split.split('\\\\')[0]\n",
    "    \n",
    "    print(file)\n",
    "\n",
    "    new_results = results.copy()\n",
    "\n",
    "    # String to dictionary\n",
    "    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n",
    "\n",
    "    # Sort with best values on top\n",
    "    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "\n",
    "    # Use best hyperparameters to create a model\n",
    "    hyperparameters = new_results.loc[0, 'hyperparameters']\n",
    "    \n",
    "    # load the appropriate model and fit on training data\n",
    "    model = load_model(model_name, hyperparameters)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # create output filename\n",
    "    new_dir = '../models/'\n",
    "    filename = new_dir + model_name + '.sav'\n",
    "    \n",
    "    # save the trained model\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NSQIP]",
   "language": "python",
   "name": "conda-env-NSQIP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
